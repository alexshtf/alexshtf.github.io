---
layout: post
title:  "Let the polynomial monster free"
tags: [polynomial,legendre,chebyshev,fourier,machine learning,artificial intelligence,ai]
description: Overparametrized polynomial regression
comments: true
image: /assets/polyedral_cone_layer.png
series: "Polynomial features in machine learning"
---
# Intro

In a recent post by Ben Recht, titled [Though Shalt Not Overfit](https://www.argmin.net/p/thou-shalt-not-overfit), Ben claims that overfitting in the way that it is colloquially described in data science and machine learning, doesn’t exist.  Indeed, there is the famous [double descent](https://en.wikipedia.org/wiki/Double_descent):  trained neural networks that have _much_ more parameters that needed to memorize the training set, tend generalize quite well. This includes most of our modern LLMs. So in many cases, models that achieve low training loss but generalize badly _not_ because their number of parameters is too high, but because it is _not high enough_!

Ben claimed that oftentimes what we call "overfitting" is just a post-hoc rationalization of a model being _wrong_, and makes us ignore the actual underlying reason. For example, it just may be the case that our model is missing some important informative features. Ben's post caused some backlash on [X](https://x.com/beenwrekt/status/1884988534307873251) with several people disagreeing, such as [here](https://x.com/KabirCreates/status/1884992728880283975), and [here](https://x.com/RVrijj/status/1885056275714629914) where the authors came back to the textbook examples of overfitting high degree polynomials. 

In this post series about polynomial features we saw that we should not fear high degree polynomials in machine learning, because they can be easily controlled via simple regularization. But what about high degree polynomials without regularization at all? Do they exhibit this double-descent, just like neural networks, and generalize well when the degree is much higher than what is needed to memorize the training data?

Well, apparently they do[^1][^2], and this is something we will explore with examples and code in this post. It turns out that even in this simple case of polynomial features in a linear models, the high-degree polynomials we see in textbooks "overfit" simply becaue their degree is high, but not _high enough_. So there is something in Ben Recht's post - the taught "balance" between model complexity and generalization is nonexistant not only for various neural network families, but also for simple polynomial fitting models! The code for this post is available in a [notebook](https://github.com/alexshtf/alexshtf.github.io/blob/master/assets/free_polynomial_monster.ipynb), and you can try it out yourself.

# Function fitting

Let's start with a small exercise - of function fitting. Here is a simple function that should be quite challenging for a polynomial to fit:

```python
import numpy as np

def func(x):
    z = np.cos(np.pi * x - 1) + 0.1 * np.cos(2 * np.pi * x + 1)
    return np.cbrt(np.abs(z)) * np.sign(z)
```

The reason is due to its shape:

```python
import matplotlib.pyplot as plt

plot_xs = np.linspace(-1, 1, 1000)
plt.plot(plot_xs, func(plot_xs))
plt.show()
```

![polyfit_challenging]({{"assets/polyfit_challenging.png" | absolute_url}})

Indeed, its slopes around $$x=-0.25$$ and $$x=0.75$$ are practically vertical, so any polynomial will have a hard time fitting it. As expected, we now generate some noisy training data:

```python
def noisy_func(x, noise=0.1):
    return func(x) + noise * np.random.randn(len(x))

np.random.seed(42)
n = 50
x = 2 * np.random.rand(n) - 1
y = noisy_func(x)
```

This is what it looks like:

```python
plt.plot(plot_xs, func(plot_xs), label='function')
plt.plot(x, y, 'o', label='data')
plt.legend()
plt.show()
```

![polyfit_challenging_data]({{"assets/polyfit_challenging_data.png" | absolute_url}})

# Fitting polynomials
To fit the polynomial to our training data, we use the standard least-squares solved from NumPy:

```python
def fit(degree, feature_matrix_fn):
    # generate polynomial features
    X = feature_matrix_fn(x, degree)
    
    # compute coefficients using the L2 loss
    poly = np.linalg.lstsq(X, y, rcond=-1)[0]
    
    # compute training error (RMSE)
    train_rmse = np.sqrt(np.mean(np.square(X @ poly - y)))
    
    # return coefficients and training error
    return poly, train_rmse
```

The `fit` function is a bit generic - it accepts a `feature_matrix_fn` that transforms each training sample $$x$$ into a vector of polynomials, such as the vector $$(1, x, x^2, \dots, x^n)$$ for polynomials of degree $$n$$. 

Beyond fitting, we will also need to measure the test error of our fit polynomials, and plot them. To that end, we simply measure the average root mean-squared error between the fit polynomial and the true function at 10,000 points:

```python
def test_fit(degree, feature_matrix_fn, coefs):
    xtest = np.linspace(-1, 1, 10000)
    ytest = feature_matrix_fn(xtest, degree) @ coefs
    test_rmse = np.sqrt(np.mean(np.square(ytest - func(xtest))))
    return xtest, ytest, test_rmse
```

But why do we need this genericity of specifying the `feature_matrix_fn`? To understand why, it's a good time to remind ourselves a thing or two about polynomial fitting we learned in the [post series]({% post_url 2024-01-21-Bernstein %}) about polynomial features.

Polynomials do not have to be necessarily represented using the standard basis $$\{1, x, x^2, \dots, x^n\}$$. For example, the polynomial

$$
p(x) = 1+2x+3x^2−5x^3
$$

an be also written as 

$$
p(x) = 2−x+2(1.5x^2−0.5)−2(2.5x^3−1.5x) \tag{L}
$$

In the first form, it's written in terms of the standard basis and has the coefficients $$(1, 2, 3, -5)$$, whereas in the second form it's written in terms of the basis $$\{1, x, 1.5x^2-0.5, 2.5x^3-1.5\}$$ and has the coefficients $$(2, -1, 2, -2)$$. The basis here is a well-known basis in numerical analysis, the [Legendre polynomial basis](https://en.wikipedia.org/wiki/Legendre_polynomials). In general a polynomial can be written as

$$
p(x) = \sum_{i=0}^n w_i P_i(x),
$$

where $$P_0(x), \dots, P_n(x)$$ are the basic polynomial functions, and $$w_0, \dots, w_n$$ are its coefficients. When learning a polynomial, the basic polynomials are the _features_, and the coefficients are the _learned parameters_ of the model.  

The reason for the genericity in `fit` is, of course, our desire fit polynomials with different bases to demonstrate a point. For least-squares fitting, the data matrix rows contain the values of the basis functions at each point in the training set. Fortunately, NumPy comes with functions to generate such matrices for a variety of polynomial bases. 


# Visualizing the fit polynomials
Now let's try to visually inspect polynomials of various degrees that fit our function. Below is a function that fits a polynomial using a given basis, computes the train and test errors, and plots the fitting results and the polynomial coefficients:

```python
def fit_and_plot(degree, feature_matrix_fn):
    poly, train_rmse = fit(degree, feature_matrix_fn)
    xtest, ytest, test_rmse = test_fit(degree, feature_matrix_fn, poly)
    coef_sum = np.sum(poly)
    fig, axs = plt.subplots(1, 2, figsize=(16, 5))
    fig.suptitle(f'Degree: {degree}, Train RMSE = {train_rmse:.4g}, Test RMSE = {test_rmse:.4g}, Coef Sum = {coef_sum:.4g}')

    axs[0].scatter(x, y)
    axs[0].plot(xtest, ytest, color='royalblue')
    axs[0].plot(xtest, func(xtest), color='red')
    axs[0].set_yscale('asinh')
    axs[0].set_title('Model')

    markerline, stemlines, baseline = axs[1].stem(poly)
    stemlines.set_linewidth(0.5)
    markerline.set_markersize(2)
    axs[1].set_title('Coefficients')
    plt.show()
```

As expected, it also has the genericity to specify our basis. So let's try plotting with the standard basis - it is implemented in the `np.polynomial.polynomial.polyvander` function. The "vander" in the name is because the polynomial feature matrix is known as the [Vandermonde matrix](https://en.wikipedia.org/wiki/Vandermonde_matrix).

```python
fit_and_plot(1, np.polynomial.polynomial.polyvander)
```

![polyfit_challenging_deg1]({{"assets/polyfit_challenging_deg1.png" | absolute_url}})

Well, surprisingly, we got a line. The test error is 0.6457. So let's try a polynomial of degree 5:

```python
fit_and_plot(5, np.polynomial.polynomial.polyvander)
```

![polyfit_challenging_std_deg5]({{"assets/polyfit_challenging_std_deg5.png" | absolute_url}})

As expected, it fits the function better, and the test error is smaller - 0.2118. Now let's try a polynomial of degree 49. This is exactly the "interpolation threshold" - we have 50 training points, so a polynomial of degree 49 can exactly memorize the training set:

```python
fit_and_plot(49, np.polynomial.polynomial.polyvander)
```

![polyfit_challenging_std_deg49]({{"assets/polyfit_challenging_std_deg49.png" | absolute_url}})

Well, it appears that we're observing what ML 101 textbooks tell us - we're overfitting! The polynomial is far away from the function, the train error is almost zero, since we're exactly fitting the training data, but the test error is $$\sim 6.7 \times 10^{7}$$! What about a polynomial of degree 10,000? Let's try!

```python
fit_and_plot(10000, np.polynomial.polynomial.polyvander)
```

![polyfit_challenging_std_deg10000]({{"assets/polyfit_challenging_std_deg10000.png" | absolute_url}})

Even worse! But we also observe something suspicious - the coefficients of the high degree polynomial are also large - so is it "overfitting", or is it something else? Well, let's try the Legendre basis. It is implemented in the `np.polynomial.legendre.legvander` function. Here is a polynomial of degree 5.

```python
fit_and_plot(5, np.polynomial.legendre.legvander)
```

![polyfit_challenging_leg_deg5]({{"assets/polyfit_challenging_leg_deg5.png" | absolute_url}})

Looks identical to the standard basis. If we think about it - it's not a surprise. There is a unique least-squares fitting polynomial of degree 5, and it doesn't matter how we represent it, standard basis, or Legendre basis. Let's try a degree of 49 - the interpolation threshold:

```python
fit_and_plot(49, np.polynomial.legendre.legvander)
```

![polyfit_challenging_leg_deg49]({{"assets/polyfit_challenging_leg_deg49.png" | absolute_url}})

Also looks almost identical to the standard basis. The train error is almost zero. The test error is awful. This is also not a surprise - there is also a unique polynomial of degree 49, the one that exactly passes through the 50 points.  So it doesn't matter which basis we use. But what happens if we crank up the degree to 10,000? Well, let's try:

```python
fit_and_plot(10000, np.polynomial.legendre.legvander)
```

![polyfit_challenging_leg_deg10000]({{"assets/polyfit_challenging_leg_deg10000.png" | absolute_url}})

Whoa! That's interesting! The polynomial, both exactly fits the training data points, but also pretty close to the true function. The test error is also not bad - $$0.2156$$. What happened to our overfitting from ML 101 textbooks? There is no regularization. No control of the degree. But "magically" our high degree polynomial is not that bad! Also look at the coefficients - they are pretty small!

We learned in this series that each basis is coupled with a corresponding "operating region" where it was designed to "work well". I am leaving the meaning of "work well" vague on purpose, but in general it means that it can accurately fit functions from a finite amount of samples in that interval.  For example, the Bernstein basis we explored in previous posts series has the interval $$[0, 1]$$ as its operating region. Moreover, its key property is that its coefficients allow control of the shape of the function. The Legendre basis we just used has the interval $$[-1, 1]$$ as its operating region, and its modus operandi is exactly the opposite of the Bernstein basis - when we _do not_ wish to impose any regularization or control at all. For the standard basis, by the way, the operating region is _the complex unit circle_.  And it's not a surprise - it's the foundation of the entire field of Fourier analysis!

It turns out that ML textbooks that use high degree polynomials to demonstrate the balance between "model complexity" and "generalization error" are _wrong_. The reason is simply the usage of the standard basis - it's operating reagion is the complex unit circle - it's a wrong choice for fitting functions of real numbers. The textbooks simply use the wrong tool for polynomial regression, or as Ben Recht pointed in his post, use the wrong features in their linear model!

In this small demo I chose the training and test samples to come from $$[-1, 1]$$ - this is exactly the operating region of the Legendre basis. In practice, when using high degree polynomial features, you have to first normalize your raw features to the operating region of your basis of choice. You can use standard tools, such as min-max scaling with clipping, or a normalization function such as $$x \to \tanh(\alpha x+ \beta)$$. You should _never_ use polynomial features outside the operating region of the basis of your choice!

# Error as a function of the degree

Here we saw examples of polynomials of degree 1, 5, 49, and 10,000. But what about the other degrees? Well, let's plot the test error as a function of the degree and see! Here is a function that plots the train and test errors for a range of polynomial degrees, and also shows the "interpolation threshold" - when the number of parameters equals the number of points:

```python
def plot_errors(feature_matrix_fn):
    # define a set of degrees that look nice in a plot - linearly spaced
    # low degrees, and geometrically spaced high degrees.
    degs = np.r_[
        np.sort(np.unique(np.linspace(1, n - 1, 15).astype(int))),
        np.sort(np.unique(np.geomspace(n+1, 100000, 20).astype(int)))
    ]

    # compute train and test errors
    train_errors = np.zeros_like(degs).astype(float)
    test_errors = np.zeros_like(degs).astype(float)
    for i, deg in enumerate(tqdm(degs)):
        poly, train_errors[i] = fit(deg, feature_matrix_fn)
        _, _, test_errors[i] = test_fit(deg, feature_matrix_fn, poly)

    # plot the train and test errors, and the interpolation threshold vertical
    # bar.
    plt.figure(figsize=(16, 6))
    plt.plot(degs, train_errors, label='Train')
    plt.plot(degs, test_errors, label='Test')
    plt.axvline(n - 1, color='royalblue', linewidth=3, linestyle='dotted',
                alpha=0.5)
    plt.axhline(np.min(test_errors), color='olive', linewidth=1, linestyle='dashed',
                alpha=0.5, label=f'Min RMSE = {np.min(test_errors):.3g}')
    plt.ylim([-1e-3, 2 * np.max(test_errors)])
    plt.yscale('asinh', linear_width=1e-3)
    plt.xscale('log')
    plt.xlabel('Degree')
    plt.ylabel('Root Mean Squared Error')
    plt.legend()
    plt.show()
```

Now let's plot the errors for the Legendre basis:

```python
plot_errors(np.polynomial.legendre.legvander)
```

![polyfit_errors_leg]({{"assets/polyfit_errors_leg.png" | absolute_url}})

We nicely see the double-descent phenomenon! At the interpolation threshold, the train error drops towards zero, but the test error skyrockets. But as the degrees increase, the test error goes down again! What about the standard basis?

```python
plot_errors(np.polynomial.polynomial.polyvander)
```

![polyfit_errors_std]({{"assets/polyfit_errors_std.png" | absolute_url}})

The behavior is identical to the Legendre basis below the interpolation threshold. This is because there is only _one_ least-squares fitting polynomial of any degree below 50. But when we cross the interpolation threshold, the train error goes towards zero, as expected, but the test error skyrockets! But this is not because of some magical phenomenon called "overfitting" - this is because the standard basis is simply the wrong tool for polynomial fitting. 

If we look again at the Legendre polynomial errors plot, we will see that still the low-degree polynomial achieves a better test error than any high-degree polynomial. But is this always the case? If you scroll up, you can see that we samples 50 training points. But if we re-run our entire simulation with 600 training points, the plot for the Legendre  errors with Legendre polynomials is quite different:

![polyfit_errors_leg_600pts]({{"assets/polyfit_errors_leg_600pts.png" | absolute_url}})

This time, the high degree polynomials generalize even better than any low degree polynomial can. This is the final nail in the coffin of the popular belief that higher model complexity leads to worse generalization! This isn't true even for polynomial models - the ones that are used to demonstrate "overfitting" in ML textbooks. So in the "big data" regime, the over-parameterized polynomials, those that memorize the training set, appear to work better as they get more and more parameters. Just like LLMs - we have huge amounts of training data, and the larger LLMs perform better!

# What makes the standard basis "bad", and the Legendre basis "good"?

There are mathematically rigorous reasons, pointed out in Schaeffer's paper on double descent[^1], but here we are aiming for a more intuitive explanation. When fitting with the standard basis, our feature matrix for polynomials of degree $$n$$ looks like this:

$$
X = \begin{pmatrix}
1 & x_1 & x_1^2 & \dots & x_1^n \\
1 & x_2 & x_2^2 & \dots & x_2^n \\
&& \vdots & & \\
1 & x_m & x_m^2 & \dots & x_m^n \\
\end{pmatrix}
$$

Intuitively, any two even powers, such as $$x^4$$ and $$x^6$$, are very similar: both grow quickly as $$x$$ gets farther from the origin. The same similarity happens to any two odd powers. This means that the columns of the matrix $$X$$ above are _highly correlated_. As degrees get higher and higher, we're practically beginning to add almost redundant columns that our linear model has to use as features. Intuitively, a lot of "non-informative" features is what makes the linear model behave badly. This is formally analyzed in Schaeffer's paper, in the form of the singular values of the matrix $$X$$.

The Legendre basis is different. Let's plot the first four polynomials, of degree 0, 1, 2, and 3:
```python
handles = plt.plot(plot_xs, np.polynomial.legendre.legvander(plot_xs, 4))
plt.legend(handles=handles, labels=[f'Degree {i}' for i in range(5)])
plt.show()
```

![legendre_basis]({{"assets/legendre_basis.png" | absolute_url}})

We can see that these polynomials _oscilate_ in $$[-1, 1]$$! Higher degrees oscilate more than lower degrees. But not only do they oscilate, but each polynomial oscilates "in different places" than the lower degrees, meaning it tries to curve up where polynomials of lower degrees curve down, and vice versa. This is formally expressed using the **orthogonality** property of Legendre polynomials - for any two Legendre basis polynomials, $$P_i(x)$$ and $$P_j(x)$$ we have:

$$
\langle P_i, P_j \rangle = \int_{-1}^1 P_i(x) P_j(x) dx = 0, \quad \text{if } i \neq j
$$

Integrals are, of course, just "infinite sums". So take uniformly sampled points $$x_1, \dots, x_m$$, we will have

$$
\sum_{k=1}^m P_i(x_k) P_j(x_k) \approx 0
$$

So if we look at the data matrix corresponding to the Legendre basis, its columns will have little chance to be correlated! Each column corresponding to a higher degree actually adding more and more information that previous columns did not have. Fitting a linear model with "informative features", of course, has a much higher chance of success.

# But extrapolation! Polynomials don't extrapolate well!
A common claim is that polynomials "go crazy" if you use them outside of the domain where your training data comes from.  Well, let's try fitting a our function using training data in $$[-0.5, 0.5]$$, and plotting it together with the fit polynomial in $$[-1, 1]$$:

```python
np.random.seed(42)
n = 25
x = np.random.rand(n) - 0.5
y = noisy_func(x)

fit_and_plot(10000, np.polynomial.legendre.legvander)
```

![polyfit_errors_leg_600pts]({{"assets/polyfit_challenging_leg_deg10000_extrapolate.png" | absolute_url}})

Indeed, we do not have training data beyond $$[-0.5, 0.5]$$, so we have no way of knowing the true behavior of the function outside of this interval. The best thing we can expect is some "graceful" behavior. Indeed, our super high degree polynomial behaves quite gracefully - it decays towards zero as we get farther away from $$[-0.5, 0.5]$$. Not bad for extrapolating!

Of course, if we go outside the operating region of the Legendre basis the polynomial will not be graceful at all. It will quickly explode towards infinity. But that's the whole point - as long as you always normalize your features to the operating region of your polynomial basis - you should expect graceful extrapolation behavior. There is actually no problem "extrapolating" outside of the domain where most of your training data came from, as long as you stay inside the operating region of your basis.


# The ML community hasn't caught up
It turns out that what we saw here has already been discovered a long time ago by the differential equations community. Many natural phenomena are modeled by differential equations, meaning equations whose variable is a function. Oftentimes, the solution cannot be expressed analytically and is approximated. 

One popular approximation method is using polynomials, which allow expressing the problem at hand using a set of linear equations in the polynomial coefficients: some of the equations stem from the laws of physics, whereas the others come from (possibly noisy) measurements. The function is then approximated by finding the coefficients stemming from exact fit to the laws of physics, and least-squares fit to the noisy data. Essentially, this is a kind of machine learning.

It turns out thay the differential equations community, and the numerical analysis community in general, already did extensive research on approximating functions using polynomials. It has been known for a very long time that families of orthogonal polynomials "work well", and this is described extensively in John Boyd's book[^3]. It is unfortunate, but knowledge doesn't always flow between scientific disciplines, and this is one of those cases. Otherwise, ML textbooks and courses wouldn't use polynomial regression to demonstrate what is "overfitting".

# Recap
Although polynomials are typically used to demonstrate the need to balance model complexity and generalization, it is a myth. It turns out that the "overfitting" phenomena in this case are just bad numerical behavior of the standard basis, and misunderstanding of the concept of the "operating region" of polynomial bases.

Moreover, typically ML theory deals alot with model classes, also called hypothesis classes. It attempts to answer questions such as: what is is the generalization power of linear regression functions? What about linear classifiers? But it's not only about model classes. The standard basis of degree 10,000 and the Legendre basis of degree 10,000 represent the same class of models - the class of polynomials of degree 10,000. But we see radically different results with two representations of the same class of models!

The representation, of course, is part of the learning algorithm itself. We can learn our 10,000 degree polynomial in many ways. This is also not a surprise - a linear classifier can be trained as logistic regression or as a support vector machine - the  learning algorithms may have different generalization power, just like two different polynomial bases.

In the next post we shall try to create a scikit-learn component that generates Legendre basis for numerical features, and use it on some real-world datasets. Let's see what happens when we crank-up the degree on something more serious than just fitting a polynomial curve to noisy samples from a function!


# References

[^1]: Schaeffer, Rylan, Mikail Khona, Zachary Robertson, Akhilan Boopathy, Kateryna Pistunova, Jason W. Rocks, Ila Rani Fiete, and Oluwasanmi Koyejo. "Double descent demystified: Identifying, interpreting & ablating the sources of a deep learning puzzle." *arXiv preprint arXiv:2303.14151* (2023).
[^2]: Philipp Benner. "Double descent". [https://github.com/pbenner/double-descent](https://github.com/pbenner/double-descent) (2022)
[^3]: John P. Boyd. "Chebyshev and Fourier Spectral Methods, 2d. edition". Dover Publishers (2001).
