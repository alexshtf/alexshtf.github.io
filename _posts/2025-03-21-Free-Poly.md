---
layout: post
title:  "Let the polynomial monster free"
tags: [polynomial,legendre,chebyshev,fourier,machine learning,artificial intelligence,ai]
description: Overparametrized polynomial regression
comments: true
image: /assets/polyedral_cone_layer.png
---
# Intro

In a recent post by Ben Recht, titled [Though Shalt Not Overfit](https://www.argmin.net/p/thou-shalt-not-overfit), Ben claims that overfitting in the way that it is colloquially described in data science and machine learning, doesn’t exist. Indeed, there is the famous [double descent](https://en.wikipedia.org/wiki/Double_descent):  trained neural networks that have _much_ more parameters that needed to memorize the training set, tend generalize quite well. This includes most of our modern LLMs. So in many cases, models that achieve low training loss but generalize badly _not_ because their number of parameters is too high, but because it is _not high enough_!

Ben Recht's post caused some backlash on [X](https://x.com/beenwrekt/status/1884988534307873251) with several people disagreeing, such as [here](https://x.com/KabirCreates/status/1884992728880283975), and [here](https://x.com/RVrijj/status/1885056275714629914) where the authors came back to the textbook examples of overfitting high degree polynomials. In our [post series]({% post_url 2024-01-21-Bernstein %}) about polynomial features we saw that we should not fear high degree polynomials in machine learning, because they can be easily controlled via simple regularization. But what about high degree polynomials without regularization at all? Do they exhibit this double-descent, just like neural networks, and generalize well when the degree is much higher than what is needed to memorize the training data?

Well, apparently they do[^1][^2], and this is something we will explore with examples and code in this post. It turns out that even in this simple case of polynomial features in a linear models, the high-degree polynomials we see in textbooks "overfit" simply becaue their degree is high, but not _high enough_. So there is something in Ben Recht's post - the taught "balance" between model complexity and generalization is nonexistant not only for various neural network families, but also for simple polynomial models!

# Function fitting

Let's start with a small exercise - of function fitting. Here is a simple function that should be quite challenging for a polynomial to fit:

```python
import numpy as np

def func(x):
    z = np.cos(np.pi * x - 1) + 0.1 * np.cos(2 * np.pi * x + 1)
    return np.cbrt(np.abs(z)) * np.sign(z)
```

The reason is due to its shape:

```python
import matplotlib.pyplot as plt

plot_xs = np.linspace(-1, 1, 1000)
plt.plot(plot_xs, func(plot_xs))
plt.show()
```

![polyfit_challenging]({{"assets/polyfit_challenging.png" | absolute_url}})

Indeed, its slopes around $$x=-0.25$$ and $$x=0.75$$ are practically vertical, so any polynomial will have a hard time fitting it. As expected, we now generate some noisy training data:

```python
def noisy_func(x, noise=0.1):
    return func(x) + noise * np.random.randn(len(x))

np.random.seed(42)
n = 50
x = 2 * np.random.rand(n) - 1
y = noisy_func(x)
```

This is what it looks like:

```python
plt.plot(plot_xs, func(plot_xs), label='function')
plt.plot(x, y, 'o', label='data')
plt.legend()
plt.show()
```

![polyfit_challenging_data]({{"assets/polyfit_challenging_data.png" | absolute_url}})

To fit the polynomial to our training data, we use the standard least-squares solved from NumPy:

```python
def fit(degree, feature_matrix_fn):
    # generate polynomial features
    X = feature_matrix_fn(x, degree)
    
    # compute coefficients using the L2 loss
    poly = np.linalg.lstsq(X, y, rcond=-1)[0]
    
    # compute training error (RMSE)
    train_rmse = np.sqrt(np.mean(np.square(X @ poly - y)))
    
    # return coefficients and training error
    return poly, train_rmse
```

The `fit` function is a bit generic - it accepts a `feature_matrix_fn` that transforms each training sample $$x$$ into a vector of polynomials, such as the vector $$(1, x, x^2, \dots, x^n)$$ for polynomials of degree $$n$$. 

Beyond fitting, we will also need to measure the test error of our fit polynomials, and plot them. To that end, we simply measure the average root mean-squared error between the fit polynomial and the true function at 10,000 points:

```python
def test_fit(degree, feature_matrix_fn, coefs):
    xtest = np.linspace(-1, 1, 10000)
    ytest = feature_matrix_fn(xtest, degree) @ coefs
    test_rmse = np.sqrt(np.mean(np.square(ytest - func(xtest))))
    return xtest, ytest, test_rmse
```

But why do we need this genericity of specifying the `feature_matrix_fn`? To understand why, it's a good time to remind ourselves a thing or two about polynomial fitting we learned in the [post series]({% post_url 2024-01-21-Bernstein %}) about polynomial features.

Polynomials do not have to be necessarily represented using the standard basis $$\{1, x, x^2, \dots, x^n\}$$. For example, the polynomial

$$
p(x) = 1+2x+3x^2−5x^3
$$

an be also written as 

$$
p(x) = 2−x+2(1.5x^2−0.5)−2(2.5x^3−1.5x) \tag{L}
$$

In the first form, it's written in terms of the standard basis and has the coefficients $$(1, 2, 3, -5)$$, whereas in the second form it's written in terms of the basis $$\{1, x, 1.5x^2-0.5, 2.5x^3-1.5\}$$ and has the coefficients $$(2, -1, 2, -2)$$. In the context of polynomial fitting - the basis comprises the features of a linear model, and the fitting procedure finds the coefficients. The reason for the genericity in `fit` is, of course, our desire fit polynomials with different bases.

We also learned in the series that each basis is coupled with a corresponding "operating region" where it was designed to "work well". I am leaving the meaning of "work well" vague on purpose, but in general it means that it can accurately fit functions from a finite amount of samples in that interval.  For example, the Bernstein basis we explored in that series has the interval $$[0, 1]$$ as its operating region. Moreover, its key property is that its coefficients allow control of the shape of the function. 

The basis we just saw in in Equation (L) above is the [Legendre basis](https://en.wikipedia.org/wiki/Legendre_polynomials). Its operating region is the interval $$[-1, 1]$$. As we shall see later in this post, it is a good fit if we _do not_ wish to control the shape of the polynomial.  For the standard basis, by the way, the operating region is _the complex unit circle_.  

It is common to claim that polynomials are bad in extrapolation. Well, it depends on what you mean by "extrapolation". If you mean evaluating the polynomial _outside of its operating region_, then yes, it is true. But that's not a surprise - a given basis of polynomials was designed with a specific operating region in mind.  I wouldn't even call it "extrapolation", in my opinion it's like dividing by zero.  But if "extrapolation" means evaluating _inside_ the operating region, but outside of the region where we have training data - then we should expect graceful behavior. Therefore, the way to use polynomial features in machine-learned models is to normalize your raw features to the operating region. Either do min-max scaling with clipping, or do some transformation, e.g. $$x \to \tanh(\alpha x + \beta)$$ will normalize a feature to the interval $$[-1, 1]$$.

Now let's try to visually inspect our function fitting using the Legendre basis. Here is a small function that plots the true function, the training data, the fit polynomial, and its coefficients:

```python
def fit_and_plot(degree, feature_matrix_fn):
    poly, train_rmse = fit(degree, feature_matrix_fn)
    xtest, ytest, test_rmse = test_fit(degree, feature_matrix_fn, poly)
    coef_sum = np.sum(poly)
    fig, axs = plt.subplots(1, 2, figsize=(16, 5))
    fig.suptitle(f'Degree: {degree}, Train RMSE = {train_rmse:.4g}, Test RMSE = {test_rmse:.4g}, Coef Sum = {coef_sum:.4g}')

    axs[0].scatter(x, y)
    axs[0].plot(xtest, ytest, color='royalblue')
    axs[0].plot(xtest, func(xtest), color='red')
    axs[0].set_yscale('asinh')
    axs[0].set_title('Model')

    markerline, stemlines, baseline = axs[1].stem(poly)
    stemlines.set_linewidth(0.5)
    markerline.set_markersize(2)
    axs[1].set_title('Coefficients')
    plt.show()
```

As expected, it also has the genericity to specify our basis. So let's try plotting with the standard basis - it is implemented in the `np.polynomial.polynomial.polyvander` function. The "vander" in the name is because the polynomial feature matrix is known as the [Vandermonde matrix](https://en.wikipedia.org/wiki/Vandermonde_matrix).

```python
fit_and_plot(1, np.polynomial.polynomial.polyvander)
```

![polyfit_challenging_deg1]({{"assets/polyfit_challenging_deg1.png" | absolute_url}})

Well, surprisingly, we got a line. The test error is 0.6457. So let's try a polynomial of degree 5:

```python
fit_and_plot(5, np.polynomial.polynomial.polyvander)
```

![polyfit_challenging_std_deg5]({{"assets/polyfit_challenging_std_deg5.png" | absolute_url}})

As expected, it fits the function better, and the test error is smaller - 0.2118. Now let's try a polynomial of degree 49, since we have 50 points:

```python
fit_and_plot(49, np.polynomial.polynomial.polyvander)
```

![polyfit_challenging_std_deg49]({{"assets/polyfit_challenging_std_deg49.png" | absolute_url}})

Well, it appears that we're observing what ML 101 textbooks tell us - we're overfitting! The polynomial is far away from the function, the train error is almost zero, since we're exactly fitting the training data, but the test error is $$\sim 6.7 \times 10^{7}$$! What about a polynomial of degree 10,000? Let's try!

```python
fit_and_plot(10000, np.polynomial.polynomial.polyvander)
```

![polyfit_challenging_std_deg10000]({{"assets/polyfit_challenging_std_deg10000.png" | absolute_url}})

Even worse! But we also observe something suspicious - the coefficients of the high degree polynomial are also large - so is it "overfitting", or is it something else? Well, let's try the Legendre basis. It is implemented in the `np.polynomial.legendre.legvander` function. Here is a polynomial of degree 5.

```python
fit_and_plot(5, np.polynomial.legendre.legvander)
```

![polyfit_challenging_leg_deg5]({{"assets/polyfit_challenging_leg_deg5.png" | absolute_url}})

Looks identical to the standard basis. If we think about it - it's not a surprise. There is a unique least-squares fitting polynomial of degree 5, and it doesn't matter how we represent it, standard basis, or Legendre basis. Let's try a degree of 49:

```python
fit_and_plot(49, np.polynomial.legendre.legvander)
```

![polyfit_challenging_leg_deg49]({{"assets/polyfit_challenging_leg_deg49.png" | absolute_url}})

Also looks almost identical to the standard basis. The train error is almost zero. The test error is awful. This is also not a surprise - there is also a unique polynomial of degree 49 - it is the interpolating polynomial, the one that exactly passes through the 50 points.  So it doesn't matter which basis we use. But what happens if we crank up the degree to 10,000? Well, let's try:

```python
fit_and_plot(10000, np.polynomial.legendre.legvander)
```

![polyfit_challenging_leg_deg10000]({{"assets/polyfit_challenging_leg_deg10000.png" | absolute_url}})

Whoa! That's interesting! The polynomial, both exactly passes through the training data points, but also pretty close to the true function. The test error is also not bad - $$0.2156$$. What happened to our overfitting from ML 101 textbooks? There is no regularization. No control of the degree. But "magically" our high degree polynomial is not that bad! Also look at the coefficients - they are pretty small!

Well, it turns out that ML textbooks that use high degree polynomials to demonstrate the balance between "model complexity" and "generalization error" are _wrong_. The reason is simply the usage of the standard basis - it's operating reagion is the complex unit circle, but it's used on those "overfitting" demonstrations on real numbers. The textbooks simply use the wrong tool for polynomial regression, or as Ben Recht pointed in his post, use the wrong features in their linear model!





**References**

[^1]: Demystify double descent paper
[^2]: Past post series about polynomial double descent
